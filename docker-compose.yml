
services:
  gpt-sovits:
    image: legwork7623/gpt-sovits:latest 
    container_name: gpt-sovits-container
    environment:
      - is_half=False
      - is_share=False
    volumes:
      - ./output:/workspace/output
      - ./logs:/workspace/logs
      - ./SoVITS_weights:/workspace/SoVITS_weights
      - ./backend/reference:/workspace/reference
    working_dir: /workspace
    ports:
      - "9880:9880"
    shm_size: 16G
    gpus: "all"
    stdin_open: true
    tty: true
    restart: unless-stopped
  ollama:
    image: ollama/ollama:0.9.0
    container_name: ollama-container
    ports:
      - "11434:11434"
    volumes:
      - ollama:/root/.ollama
    environment:
      - OLLAMA_KEEP_ALIVE=24h
      - OLLAMA_HOST=0.0.0.0
    deploy:
      resources:
        reservations:
          devices:
            - capabilities: [gpu]
    tty: true
    entrypoint: ["/bin/ollama", "serve"]
    restart: unless-stopped
  backend:
    #image: legwork7623/kurisu-assistant-core:latest
    build:
      context: ./backend
      dockerfile: Dockerfile
    container_name: core-container
    ports:
      - "15597:15597"
    depends_on:
      - ollama
      - gpt-sovits
    gpus: "all"
    environment:
      - TTS_API_URL=http://gpt-sovits-container:9880/tts
      - LLM_API_URL=http://ollama-container:11434
    volumes:
      - ./backend/whisper-finetuned:/app/whisper-finetuned
      - ./backend/configs:/app/configs
    tty: true
    restart: unless-stopped
  # mcp:
  #   build: 
  #     context: ./mcp
  #     dockerfile: Dockerfile
  #   container_name: mcp-container
  #   ports:
  #     - "14250:14250"
  #   volumes:
  #     - ./mcp/:/app/
  #   tty: true

  # nginx:
  #   image: nginx:stable-alpine
  #   container_name: nginx-container
  #   volumes:
  #     - ./configs/nginx/conf.d/nginx.conf:/etc/nginx/conf.d/nginx.conf:ro
  #   ports:
  #     - "14251:14251"
  #   depends_on:
  #     - backend
  #   restart: unless-stopped

networks:
  default:

volumes:
  ollama: